{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import mysql.connector\n",
    "from mysql.connector import Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Extract: Read the data from the CSV file\n",
    "def extract(file_path):\n",
    "    # Load data from CSV file into pandas DataFrame\n",
    "    if isinstance(file_path, str):\n",
    "        df = pd.read_csv(file_path)\n",
    "    else :\n",
    "        return file_path\n",
    "    return df\n",
    "\n",
    "# 2. Transform: Clean and transform the data (you can add more transformation logic as needed)\n",
    "def transform(df):\n",
    "    if 'order_id' in df.columns :\n",
    "        df.drop('order_id',axis= 1,inplace = True)\n",
    "    # Example transformation: Convert 'is_active' to boolean if it's not\n",
    "    df['is_active'] = 1\n",
    "    \n",
    "    # You can add more transformations here as needed\n",
    "    return df\n",
    "\n",
    "def load(df, connection_params, table_name):\n",
    "    try:\n",
    "        # Establish a connection using the connection parameters\n",
    "        conn = mysql.connector.connect(**connection_params)\n",
    "        cursor = conn.cursor()\n",
    "        # Iterate over each row of the DataFrame\n",
    "        for _, row in df.iterrows():\n",
    "            # Filter out the columns with null (None) values in the row\n",
    "            non_null_row = row.dropna()  # This drops columns with null values in the current row\n",
    "            \n",
    "            if non_null_row.empty:\n",
    "                print(f\"Skipping row with no non-null data: {row}\")\n",
    "                continue\n",
    "\n",
    "            # Dynamically generate the columns and placeholders for the current row\n",
    "            columns = ', '.join(non_null_row.index)  # Get the non-null column names\n",
    "            placeholders = ', '.join(['%s'] * len(non_null_row))  # Create placeholders for non-null values\n",
    "            values = tuple(non_null_row.values)  # Get the non-null values as a tuple\n",
    "\n",
    "            # Construct the INSERT query for the current row\n",
    "            insert_query = f\"INSERT INTO {table_name} ({columns}) VALUES ({placeholders})\"\n",
    "            # Execute the insert query for the current row\n",
    "            cursor.execute(insert_query, values)\n",
    "        \n",
    "        # Commit the transaction after inserting all rows\n",
    "        conn.commit()\n",
    "        print(f\"Data loaded successfully into the table: {table_name}\")\n",
    "    except Error as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    finally:\n",
    "        if conn.is_connected():\n",
    "            cursor.close()\n",
    "            conn.close()\n",
    "\n",
    "# Main dim_dim_dim_dim_etl Function\n",
    "def dim_etl(file_path, connection_params, table_name):\n",
    "    # Extract\n",
    "    df = extract(file_path)\n",
    "    \n",
    "    # Transform\n",
    "    df_transformed = transform(df)\n",
    "    \n",
    "    # Load\n",
    "    load(df_transformed, connection_params, table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connection parameters for the database\n",
    "connection_params = {\n",
    "    'host': 'localhost',\n",
    "    'database': 'e-commerce-dwh',\n",
    "    'user': 'root',\n",
    "    'password': '0000'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './data/user_dataset.csv'\n",
    "\n",
    "# # Name of the table you want to load data into\n",
    "table_name = 'dim_user'\n",
    "\n",
    "# # Run the ETL process\n",
    "dim_etl(file_path, connection_params, table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Product Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully into the table: dim_product\n"
     ]
    }
   ],
   "source": [
    "file_path = './data/products_dataset.csv'\n",
    "\n",
    "# # Name of the table you want to load data into\n",
    "table_name = 'dim_product'\n",
    "\n",
    "# # Run the dim_etl process\n",
    "dim_etl(file_path, connection_params, table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seller Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully into the table: dim_seller\n"
     ]
    }
   ],
   "source": [
    "file_path = './data/seller_dataset.csv'\n",
    "\n",
    "# # Name of the table you want to load data into\n",
    "table_name = 'dim_seller'\n",
    "\n",
    "# # Run the ETL process\n",
    "dim_etl(file_path, connection_params, table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Payment Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully into the table: dim_payment\n"
     ]
    }
   ],
   "source": [
    "file_path = './data/payment_dataset.csv'\n",
    "\n",
    "# # Name of the table you want to load data into\n",
    "table_name = 'dim_payment'\n",
    "\n",
    "# # Run the ETL process\n",
    "dim_etl(file_path, connection_params, table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FeedBack Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully into the table: dim_feedback\n"
     ]
    }
   ],
   "source": [
    "file_path = './data/feedback_dataset.csv'\n",
    "\n",
    "# # Name of the table you want to load data into\n",
    "table_name = 'dim_feedback'\n",
    "\n",
    "# # Run the ETL process\n",
    "dim_etl(file_path, connection_params, table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_dates(df, date_columns):\n",
    "    \"\"\"\n",
    "    This function accepts a DataFrame and a list of date column names, \n",
    "    and returns a DataFrame with unique date values from those columns,\n",
    "    with seconds set to 00.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: pandas DataFrame containing the data\n",
    "    - date_columns: list of column names that contain date values\n",
    "    \n",
    "    Returns:\n",
    "    - unique_dates_df: pandas DataFrame containing unique date values from the specified columns\n",
    "    \"\"\"\n",
    "    # Create an empty list to store unique dates\n",
    "    unique_dates = pd.Series(dtype='datetime64[ns]')\n",
    "\n",
    "    # Iterate over the date columns and extract unique values\n",
    "    for column in date_columns:\n",
    "        column_dates = pd.to_datetime(df[column], errors='coerce').apply(lambda x: x.replace(second=0, microsecond=0)).dropna().unique()\n",
    "        # Combine with existing unique dates\n",
    "        unique_dates = pd.concat([unique_dates, pd.Series(column_dates)], ignore_index=True)\n",
    "\n",
    "    # Remove duplicates and reset the index\n",
    "    unique_dates = unique_dates.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    # Convert the unique dates to a DataFrame\n",
    "    unique_dates_df = pd.DataFrame(unique_dates, columns=['full_timestamp'])\n",
    "    \n",
    "    return unique_dates_df\n",
    "\n",
    "\n",
    "def add_date_attributes(unique_dates):\n",
    "    \"\"\"\n",
    "    Adds date-related attributes to the unique dates.\n",
    "    \n",
    "    Parameters:\n",
    "    - unique_dates: pandas DataFrame containing unique date values\n",
    "    \n",
    "    Returns:\n",
    "    - date_dimension_df: pandas DataFrame containing the full set of date attributes\n",
    "    \"\"\"\n",
    "    unique_dates['full_timestamp'] = pd.to_datetime(unique_dates['full_timestamp'])\n",
    "    # Creating a dictionary to store date attributes\n",
    "    date_attributes = {\n",
    "        'full_timestamp': unique_dates['full_timestamp'],\n",
    "        'year': unique_dates['full_timestamp'].dt.year,\n",
    "        'quarter': unique_dates['full_timestamp'].dt.quarter,\n",
    "        'month': unique_dates['full_timestamp'].dt.month,\n",
    "        'month_name': unique_dates['full_timestamp'].dt.strftime('%B'),\n",
    "        'day': unique_dates['full_timestamp'].dt.day,\n",
    "        'day_of_week': unique_dates['full_timestamp'].dt.weekday + 1,  # Monday=1, Sunday=7\n",
    "        'day_of_week_name': unique_dates['full_timestamp'].dt.strftime('%A'),\n",
    "        'hour': unique_dates['full_timestamp'].dt.hour,\n",
    "        'minute': unique_dates['full_timestamp'].dt.minute,\n",
    "        'week_of_year': unique_dates['full_timestamp'].dt.isocalendar().week\n",
    "    }\n",
    "    \n",
    "    # Convert the dictionary to a DataFrame\n",
    "    date_dimension_df = pd.DataFrame(date_attributes)\n",
    "    \n",
    "    return date_dimension_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(199926,)\n",
      "(275575,)\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv('./data/order_dataset.csv')\n",
    "df2 = pd.read_csv('./data/order_item_dataset.csv')\n",
    "df = pd.merge(df1, df2, on='order_id', how='inner')\n",
    "unique_dates_df = get_unique_dates(df, ['order_date','order_approved_date','pickup_date','delivered_date','pickup_limit_date'])\n",
    "# date_df = add_date_attributes(unique_dates_df)\n",
    "# dim_etl(date_df, connection_params, 'dim_date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fact Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_id</th>\n",
       "      <th>user_name</th>\n",
       "      <th>order_status</th>\n",
       "      <th>order_date</th>\n",
       "      <th>order_approved_date</th>\n",
       "      <th>pickup_date</th>\n",
       "      <th>delivered_date</th>\n",
       "      <th>estimated_time_delivery</th>\n",
       "      <th>order_item_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>seller_id</th>\n",
       "      <th>pickup_limit_date</th>\n",
       "      <th>price</th>\n",
       "      <th>shipping_cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e481f51cbdc54678b7cc49136f2d6af7</td>\n",
       "      <td>7c396fd4830fd04220f754e42b4e5bff</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2017-10-02 10:56:33</td>\n",
       "      <td>2017-10-02 11:07:15</td>\n",
       "      <td>2017-10-04 19:55:00</td>\n",
       "      <td>2017-10-10 21:25:13</td>\n",
       "      <td>2017-10-18 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>87285b34884572647811a353c7ac498a</td>\n",
       "      <td>3504c0cb71d7fa48d967e0e4c94d59d9</td>\n",
       "      <td>2017-10-06 11:07:15</td>\n",
       "      <td>29990.0</td>\n",
       "      <td>8720.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           order_id                         user_name  \\\n",
       "0  e481f51cbdc54678b7cc49136f2d6af7  7c396fd4830fd04220f754e42b4e5bff   \n",
       "\n",
       "  order_status           order_date  order_approved_date          pickup_date  \\\n",
       "0    delivered  2017-10-02 10:56:33  2017-10-02 11:07:15  2017-10-04 19:55:00   \n",
       "\n",
       "        delivered_date estimated_time_delivery  order_item_id  \\\n",
       "0  2017-10-10 21:25:13     2017-10-18 00:00:00              1   \n",
       "\n",
       "                         product_id                         seller_id  \\\n",
       "0  87285b34884572647811a353c7ac498a  3504c0cb71d7fa48d967e0e4c94d59d9   \n",
       "\n",
       "     pickup_limit_date    price  shipping_cost  \n",
       "0  2017-10-06 11:07:15  29990.0         8720.0  "
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv('./data/order_dataset.csv')\n",
    "df2 = pd.read_csv('./data/order_item_dataset.csv')\n",
    "df = pd.merge(df1, df2, on='order_id', how='inner')\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_user_key(order_df,date_columns, connection_params):\n",
    "    \"\"\"\n",
    "    Map user_name in order_df to user_key from dim_user and update order_df.\n",
    "    \n",
    "    Parameters:\n",
    "    - order_df: pandas DataFrame containing the order data with user_name.\n",
    "    - connection_params: dictionary with MySQL connection parameters.\n",
    "    \n",
    "    Returns:\n",
    "    - Updated order_df with user_key instead of user_name.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Connect to the database\n",
    "        conn = mysql.connector.connect(**connection_params)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Fetch user_key and user_id from dim_user\n",
    "        query = \"SELECT user_id, user_name FROM dim_user\"\n",
    "        cursor.execute(query)\n",
    "        user_mapping = {user_id: user_key for user_key, user_id in cursor.fetchall()}\n",
    "        # Map user_name (order_df) to user_key (dim_user) using user_id\n",
    "        order_df['user_id'] = order_df['user_name'].map(user_mapping).apply(lambda x: pd.NA if pd.isna(x) else int(x))  # Use -1 for unknown users\n",
    "\n",
    "        # Fetch seller_key and seller_id from dim_seller\n",
    "        query = \"SELECT seller_id, seller_key FROM dim_seller\"\n",
    "        cursor.execute(query)\n",
    "        seller_mapping = {seller_key: seller_id for seller_key, seller_id in cursor.fetchall()}\n",
    "        order_df['seller_id'] = order_df['seller_id'].map(seller_mapping).apply(lambda x: pd.NA if pd.isna(x) else int(x))\n",
    "\n",
    "        # Fetch product_key and user_id from dim_user\n",
    "        query = \"SELECT product_id, product_key FROM dim_product\"\n",
    "        cursor.execute(query)\n",
    "        product_mapping = {product_key: product_id for product_key, product_id in cursor.fetchall()}\n",
    "        order_df['product_id'] = order_df['product_id'].map(product_mapping).apply(lambda x: pd.NA if pd.isna(x) else int(x))\n",
    "\n",
    "        # Fetch id and fulldate from dim_date\n",
    "        query = \"SELECT date_id, full_timestamp FROM dim_date\"\n",
    "        cursor.execute(query)\n",
    "        date_mapping = {full_timestamp: date_id for date_id,full_timestamp in cursor.fetchall()}\n",
    "\n",
    "        for column in date_columns:\n",
    "            order_df[column] = pd.to_datetime(df[column]).apply(lambda x: x.replace(second=0, microsecond=0))\n",
    "            order_df[f'{column}_id'] = order_df[column].map(date_mapping).apply(lambda x: pd.NA if pd.isna(x) else int(x))\n",
    "        \n",
    "        \n",
    "\n",
    "        order_df.drop(columns=['user_name']+date_columns, inplace=True)  # Drop user_name column\n",
    "        print(\"dimensions mapped successfully.\")\n",
    "        return order_df\n",
    "\n",
    "    except Error as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    finally:\n",
    "        if conn.is_connected():\n",
    "            cursor.close()\n",
    "            conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimensions mapped successfully.\n"
     ]
    }
   ],
   "source": [
    "date_columns = ['order_date', 'order_approved_date', 'pickup_date', 'delivered_date','pickup_limit_date']\n",
    "df = map_user_key(df,date_columns, connection_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback_df = pd.read_csv('./data/feedback_dataset.csv')\n",
    "payment_df = pd.read_csv('./data/payment_dataset.csv')\n",
    "\n",
    "def add_feedback_and_payment(feedback_df,payment_df,df):\n",
    "\n",
    "    \"\"\"\n",
    "    Adds feedback and payment information to the main DataFrame.\n",
    "    This function takes three DataFrames: feedback_df, payment_df, and df. It adds two new columns to df:\n",
    "    'feedback_id' and 'payment_id', which are mapped from the feedback_df and payment_df respectively based on 'order_id'.\n",
    "    Parameters:\n",
    "    feedback_df (pd.DataFrame): DataFrame containing feedback information with 'order_id'.\n",
    "    payment_df (pd.DataFrame): DataFrame containing payment information with 'order_id'.\n",
    "    df (pd.DataFrame): Main DataFrame to which feedback and payment information will be added.\n",
    "    Returns:\n",
    "    None: The function modifies the input DataFrame df in place.\n",
    "    \"\"\"\n",
    "\n",
    "    feedback_df['feedback_key'] = feedback_df.index + 1\n",
    "    feedback_mapper = {row['order_id']: row['feedback_key'] for _, row in feedback_df.iterrows()}\n",
    "    df['feedback_id'] = df['order_id'].map(feedback_mapper).apply(lambda x: pd.NA if pd.isna(x) else int(x))\n",
    "    payment_df['payment_key'] = payment_df.index + 1\n",
    "    payment_mapper = {row['order_id']: row['payment_key'] for _, row in payment_df.iterrows()} \n",
    "    df['payment_id'] = df['order_id'].map(payment_mapper).apply(lambda x: pd.NA if pd.isna(x) else int(x))\n",
    "\n",
    "add_feedback_and_payment(feedback_df,payment_df,df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "def transform_fact(df):\n",
    "    duplicate_order_ids = df[df.duplicated(subset=['order_id'], keep=False)]\n",
    "    print(\"Duplicate order_ids:\")\n",
    "    duplicate_order_ids.head()\n",
    "    # Drop duplicates if all columns match except order_item_id\n",
    "    df_deduped = df.drop_duplicates(subset=[col for col in df.columns if col != 'order_item_id'])\n",
    "\n",
    "    # Show shape before and after dropping duplicates\n",
    "    print(\"Shape before dropping duplicates:\", df.shape)\n",
    "    print(\"Shape after dropping duplicates:\", df_deduped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate order_ids:\n",
      "Shape before dropping duplicates: (112650, 16)\n",
      "Shape after dropping duplicates: (102425, 16)\n",
      "Data loaded successfully into the table: fact_order\n"
     ]
    }
   ],
   "source": [
    "transform_fact(df)\n",
    "load(df, connection_params, 'fact_order')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
